{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88291,"databundleVersionId":10117909,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \n\nimport pandas as pd \n\nimport matplotlib.pyplot as plt \n\nimport random\n\nimport os","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:36:03.89872Z","iopub.execute_input":"2024-11-11T13:36:03.89909Z","iopub.status.idle":"2024-11-11T13:36:03.90367Z","shell.execute_reply.started":"2024-11-11T13:36:03.899058Z","shell.execute_reply":"2024-11-11T13:36:03.9027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/train.csv')\n\n# Define path to video clips\n\nvideo_dir = '/kaggle/input/PES-ml-hack-link1/train_videos'\n\n\n\n\n\n# Function to get video file path from IDs\n\ndef get_video_clip_path(row):\n\n    dialogue_id = row['Dialogue_ID']\n\n    utterance_id = row['Utterance_ID']\n\n    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n\n    return os.path.join(video_dir, filename)\n\n\n\n# Apply the function to get file paths for each sampled clip\n\ntrain_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n\n\n\n# Check sample paths\n\nprint(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:36:04.180616Z","iopub.execute_input":"2024-11-11T13:36:04.181534Z","iopub.status.idle":"2024-11-11T13:36:04.215377Z","shell.execute_reply.started":"2024-11-11T13:36:04.181485Z","shell.execute_reply":"2024-11-11T13:36:04.214504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:36:04.635496Z","iopub.execute_input":"2024-11-11T13:36:04.636359Z","iopub.status.idle":"2024-11-11T13:36:04.642113Z","shell.execute_reply.started":"2024-11-11T13:36:04.636315Z","shell.execute_reply":"2024-11-11T13:36:04.641215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define path to video clips\n\ntest_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/test.csv', encoding = 'cp1252')\n\nvideo_dir = '/kaggle/input/PES-ml-hack-link1/test_videos'\n\n\n\n\n\n# Function to get video file path from IDs\n\ndef get_video_clip_path(row):\n\n    dialogue_id = row['Dialogue_ID']\n\n    utterance_id = row['Utterance_ID']\n\n    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n\n    return os.path.join(video_dir, filename)\n\n\n\n# Apply the function to get file paths for each sampled clip\n\ntest_df['video_clip_path'] = test_df.apply(get_video_clip_path, axis=1)\n\n\n\n# Check sample paths\n\nprint(test_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:36:05.081218Z","iopub.execute_input":"2024-11-11T13:36:05.081988Z","iopub.status.idle":"2024-11-11T13:36:05.097758Z","shell.execute_reply.started":"2024-11-11T13:36:05.081946Z","shell.execute_reply":"2024-11-11T13:36:05.096869Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Emotion recognition from a video","metadata":{}},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:36:06.763966Z","iopub.execute_input":"2024-11-11T13:36:06.764345Z","iopub.status.idle":"2024-11-11T13:36:06.779134Z","shell.execute_reply.started":"2024-11-11T13:36:06.764308Z","shell.execute_reply":"2024-11-11T13:36:06.778182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure StartTime and EndTime are in a datetime or timedelta format\ntrain_df['StartTime'] = pd.to_datetime(train_df['StartTime'])\ntrain_df['EndTime'] = pd.to_datetime(train_df['EndTime'])\n\n# Calculate duration as the difference between EndTime and StartTime\ntrain_df['duration'] = (train_df['EndTime'] - train_df['StartTime']).dt.total_seconds()\n\n# Optionally, drop the StartTime and EndTime columns if you no longer need them\ntrain_df.drop(columns=['StartTime', 'EndTime'], inplace=True)\n\nprint(train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:56:28.484996Z","iopub.execute_input":"2024-11-11T14:56:28.486222Z","iopub.status.idle":"2024-11-11T14:56:28.603828Z","shell.execute_reply.started":"2024-11-11T14:56:28.486157Z","shell.execute_reply":"2024-11-11T14:56:28.602933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Create a count of each emotion per speaker\nemotion_profile = train_df.groupby(['Speaker', 'Emotion']).size().unstack(fill_value=0)\n\n# Step 2: Calculate the frequency (or proportion) of each emotion per speaker\nemotion_profile = emotion_profile.div(emotion_profile.sum(axis=1), axis=0)\n\nprint(emotion_profile.head(n=20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:59:16.320289Z","iopub.execute_input":"2024-11-11T14:59:16.32075Z","iopub.status.idle":"2024-11-11T14:59:16.336427Z","shell.execute_reply.started":"2024-11-11T14:59:16.320707Z","shell.execute_reply":"2024-11-11T14:59:16.335617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge the emotional profile back to the main DataFrame based on Speaker\ntrain_df = train_df.merge(emotion_profile, on='Speaker', how='left', suffixes=('', '_profile'))\n# Drop the specified emotion columns\n# train_df.drop(columns=['anger', 'joy', 'neutral', 'sadness', 'surprise'], inplace=True)\ntrain_df.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:57:13.162556Z","iopub.execute_input":"2024-11-11T14:57:13.163375Z","iopub.status.idle":"2024-11-11T14:57:13.19456Z","shell.execute_reply.started":"2024-11-11T14:57:13.163332Z","shell.execute_reply":"2024-11-11T14:57:13.193471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For each row, find the emotion with the highest probability in the profile columns\nemotion_profile_columns = ['anger', 'joy', 'neutral', 'sadness', 'surprise']\ntrain_df['predicted_emotion'] = train_df[emotion_profile_columns].idxmax(axis=1)\n\n# Check how often the highest probability emotion matches the actual emotion\naccuracy = (train_df['predicted_emotion'] == train_df['Emotion']).mean()\n\nprint(f\"Percentage of times the highest probability matches the actual emotion: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:57:26.620134Z","iopub.execute_input":"2024-11-11T14:57:26.621428Z","iopub.status.idle":"2024-11-11T14:57:26.631094Z","shell.execute_reply.started":"2024-11-11T14:57:26.621369Z","shell.execute_reply":"2024-11-11T14:57:26.630116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Emotion distribution by Season\nimport seaborn as sns\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Season', hue='Emotion', data=train_df)\nplt.title('Emotion Distribution by Season')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:57:48.313083Z","iopub.execute_input":"2024-11-11T14:57:48.3139Z","iopub.status.idle":"2024-11-11T14:57:48.997271Z","shell.execute_reply.started":"2024-11-11T14:57:48.313858Z","shell.execute_reply":"2024-11-11T14:57:48.996301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boxplot for duration distribution by emotion\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Emotion', y='duration', data=train_df)\nplt.title('Duration Distribution by Emotion')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:58:02.377117Z","iopub.execute_input":"2024-11-11T14:58:02.378063Z","iopub.status.idle":"2024-11-11T14:58:02.686784Z","shell.execute_reply.started":"2024-11-11T14:58:02.37802Z","shell.execute_reply":"2024-11-11T14:58:02.685803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text/ Utterance Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\n\n# Load the dataset\ntrain_data = train_df  # Assuming you already have the train_df loaded\n\n# Load the pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenization function\ndef tokenize_function(text):\n    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n\n# Tokenize the 'Utterance' column\ntokenized_data = train_data['Utterance'].apply(tokenize_function)\n\n# Extract input_ids and attention_mask\ninput_ids = torch.cat([x['input_ids'] for x in tokenized_data], dim=0)\nattention_mask = torch.cat([x['attention_mask'] for x in tokenized_data], dim=0)\n\n# Map the labels to numeric values\nemotion_map = {'anger': 0, 'joy': 1, 'neutral': 2, 'sadness': 3, 'surprise': 4}\nlabels_text = train_data['Emotion'].map(emotion_map).values\n\n# Check if CUDA is available and select the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained BERT model and move to the appropriate device\nbert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n\n# Wrap the model with DataParallel to utilize multiple GPUs\nbert_model = torch.nn.DataParallel(bert_model)\n\n# Feature extraction function\ndef extract_bert_features(input_ids, attention_mask):\n    input_ids = input_ids.to(device)\n    attention_mask = attention_mask.to(device)\n    \n    with torch.no_grad():\n        outputs = bert_model(input_ids, attention_mask=attention_mask)\n        last_hidden_states = outputs.last_hidden_state\n        cls_embeddings = last_hidden_states[:, 0, :]  # [CLS] token embeddings\n    return cls_embeddings\n\n# Extract features for the entire dataset\nfeatures = extract_bert_features(input_ids, attention_mask)\n\n# Convert features to numpy arrays for further processing\nfeatures_text = features.cpu().numpy()  # Move to CPU before converting to numpy\n\n# Output the features and labels\nprint(\"Features shape:\", features_text.shape)\nprint(\"Labels shape:\", labels_text.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:20:54.654063Z","iopub.execute_input":"2024-11-11T14:20:54.654491Z","iopub.status.idle":"2024-11-11T14:20:59.528216Z","shell.execute_reply.started":"2024-11-11T14:20:54.654452Z","shell.execute_reply":"2024-11-11T14:20:59.527217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:20:59.53019Z","iopub.execute_input":"2024-11-11T14:20:59.530944Z","iopub.status.idle":"2024-11-11T14:20:59.540206Z","shell.execute_reply.started":"2024-11-11T14:20:59.530897Z","shell.execute_reply":"2024-11-11T14:20:59.539112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.neural_network import MLPClassifier\nX_train_features = features_text  # BERT embeddings as features\ny_train = labels_text  # Corresponding labels\n\n# Initialize and train the MLP classifier\nmlp_model = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=1000, activation='relu', \n                          early_stopping=True, learning_rate_init=0.001, solver='adam', random_state=42)\nmlp_model.fit(X_train_features, y_train)\n\n# If you have a validation set, use it for evaluation\n# For demonstration purposes, we'll assume the same training set is used for validation\n# If you have separate validation data, replace `X_train_features` and `y_train` with `X_val_features` and `y_val` respectively\n\n# Make predictions on the training set (or validation set)\ny_pred_mlp = mlp_model.predict(X_train_features)  # Use X_val_features for validation data if applicable\n\n# Evaluate the model\nprint(\"MLP Accuracy:\", accuracy_score(y_train, y_pred_mlp))  # Use y_val for validation labels if applicable\nprint(\"MLP Classification Report:\\n\", classification_report(y_train, y_pred_mlp))  # Use","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:21:09.574101Z","iopub.execute_input":"2024-11-11T14:21:09.574864Z","iopub.status.idle":"2024-11-11T14:21:10.229206Z","shell.execute_reply.started":"2024-11-11T14:21:09.574821Z","shell.execute_reply":"2024-11-11T14:21:10.227938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AUDIO Model ","metadata":{}},{"cell_type":"code","source":"!pip install moviepy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:40:01.539387Z","iopub.execute_input":"2024-11-11T13:40:01.540108Z","iopub.status.idle":"2024-11-11T13:40:28.259311Z","shell.execute_reply.started":"2024-11-11T13:40:01.540063Z","shell.execute_reply":"2024-11-11T13:40:28.258221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import moviepy.editor as mp\nfrom pydub import AudioSegment\nimport numpy as np\nimport librosa\n\ndef extract_audio_features_from_video(video_path):\n    # Load the video file using moviepy\n    video_clip = mp.VideoFileClip(video_path)\n    \n    # Extract the audio from the video clip and save it as a temporary in-memory audio file (wav format)\n    audio = video_clip.audio\n    audio_path = \"/tmp/temp_audio.wav\"  # Temporary file for the audio\n    \n    # Write the audio to the temporary file\n    audio.write_audiofile(audio_path, codec='pcm_s16le')\n\n    # Now, use pydub to load the audio from the temporary wav file\n    audio_segment = AudioSegment.from_wav(audio_path)\n\n    # Convert the audio to numpy array (for librosa compatibility)\n    samples = np.array(audio_segment.get_array_of_samples())\n    \n    # If the audio is stereo, take the mean of both channels to convert to mono\n    if audio_segment.channels > 1:\n        samples = samples.reshape((-1, audio_segment.channels)).mean(axis=1)\n    \n    # Extract audio features using librosa\n    sample_rate = audio_segment.frame_rate  # Get the sample rate from the audio file\n    mfccs = np.mean(librosa.feature.mfcc(y=samples, sr=sample_rate, n_mfcc=13).T, axis=0)\n    chroma = np.mean(librosa.feature.chroma_stft(y=samples, sr=sample_rate).T, axis=0)\n    mel = np.mean(librosa.feature.melspectrogram(y=samples, sr=sample_rate).T, axis=0)\n    contrast = np.mean(librosa.feature.spectral_contrast(y=samples, sr=sample_rate).T, axis=0)\n    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(samples), sr=sample_rate).T, axis=0)\n\n    # Combine all extracted features\n    features = np.hstack([mfccs, chroma, mel, contrast, tonnetz])\n    \n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:40:28.261798Z","iopub.execute_input":"2024-11-11T13:40:28.262307Z","iopub.status.idle":"2024-11-11T13:40:28.688816Z","shell.execute_reply.started":"2024-11-11T13:40:28.262261Z","shell.execute_reply":"2024-11-11T13:40:28.687784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from joblib import Parallel, delayed\n\ndef extract_features_for_row(row):\n    video_path = row['video_clip_path']\n    return extract_audio_features_from_video(video_path)\n\ndef extract_features_in_parallel(df):\n    results = Parallel(n_jobs=-1)(delayed(extract_features_for_row)(row) for _, row in df.iterrows())\n    \n    features = []\n    labels = []\n    for i, result in enumerate(results):\n        features.append(result)\n        labels.append(df.iloc[i]['Emotion'])\n    \n    return np.array(features), np.array(labels)\n\n# Usage example:\nfeatures_all, labels_all = extract_features_in_parallel(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:42:45.547632Z","iopub.execute_input":"2024-11-11T13:42:45.548608Z","iopub.status.idle":"2024-11-11T13:47:08.022225Z","shell.execute_reply.started":"2024-11-11T13:42:45.548564Z","shell.execute_reply":"2024-11-11T13:47:08.021005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# Split data into training and validation sets (e.g., 80% train, 20% validation)\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(features_all, labels_all, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclassifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nclassifier.fit(X_train_split, y_train_split)\n\n# Make predictions on the validation set\ny_pred = classifier.predict(X_val_split)\n\n# Evaluate accuracy\naccuracy = accuracy_score(y_val_split, y_pred)\nprint(f\"Validation Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:50:52.846793Z","iopub.execute_input":"2024-11-11T13:50:52.847818Z","iopub.status.idle":"2024-11-11T13:50:53.643586Z","shell.execute_reply.started":"2024-11-11T13:50:52.847765Z","shell.execute_reply":"2024-11-11T13:50:53.642685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Video model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nimport cv2\nfrom imblearn.over_sampling import SMOTE\n# Video feature extraction function (Increase number of frames)\ndef extract_video_features(video_path, num_frames=10):  # Increased number of frames\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            # Resize the frame for uniform feature extraction\n            frame = cv2.resize(frame, (64, 64))  # e.g., 64x64\n            frames.append(frame)\n    cap.release()\n\n    # Flatten and stack frames to a single vector\n    return np.array(frames).flatten()\n\n# Extract video features for train and test datasets\ntrain_video_features = np.array([extract_video_features(path) for path in train_df['video_clip_path']])\ntest_video_features = np.array([extract_video_features(path) for path in test_df['video_clip_path']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:52:53.454805Z","iopub.execute_input":"2024-11-11T13:52:53.455212Z","iopub.status.idle":"2024-11-11T14:06:38.20501Z","shell.execute_reply.started":"2024-11-11T13:52:53.455173Z","shell.execute_reply":"2024-11-11T14:06:38.203862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_video_features.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:09:51.493362Z","iopub.execute_input":"2024-11-11T14:09:51.493792Z","iopub.status.idle":"2024-11-11T14:09:51.500221Z","shell.execute_reply.started":"2024-11-11T14:09:51.49375Z","shell.execute_reply":"2024-11-11T14:09:51.499338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.neural_network import MLPClassifier\nX_train_features = train_video_features  # BERT embeddings as features\ny_train = labels_text  # Corresponding labels\n\n# Initialize and train the MLP classifier\nmlp_model_vid = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=1000, activation='relu', \n                          early_stopping=True, learning_rate_init=0.001, solver='adam', random_state=42)\nmlp_model_vid.fit(X_train_features, y_train)\n\n# If you have a validation set, use it for evaluation\n# For demonstration purposes, we'll assume the same training set is used for validation\n# If you have separate validation data, replace `X_train_features` and `y_train` with `X_val_features` and `y_val` respectively\n\n# Make predictions on the training set (or validation set)\ny_pred_mlp = mlp_model_vid.predict(X_train_features)  # Use X_val_features for validation data if applicable\n\n# Evaluate the model\nprint(\"MLP Accuracy:\", accuracy_score(y_train, y_pred_mlp))  # Use y_val for validation labels if applicable\nprint(\"MLP Classification Report:\\n\", classification_report(y_train, y_pred_mlp))  # Use","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:13:27.214786Z","iopub.execute_input":"2024-11-11T14:13:27.215759Z","iopub.status.idle":"2024-11-11T14:14:52.651271Z","shell.execute_reply.started":"2024-11-11T14:13:27.215717Z","shell.execute_reply":"2024-11-11T14:14:52.650092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fusion\n","metadata":{}},{"cell_type":"code","source":"print(\"Audio features shape:\", features_all.shape)\nprint(\"BERT features shape:\", X_train_features.shape)\nprint(\"Video features shape:\", train_video_features.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:51:29.749393Z","iopub.execute_input":"2024-11-11T14:51:29.750006Z","iopub.status.idle":"2024-11-11T14:51:29.756073Z","shell.execute_reply.started":"2024-11-11T14:51:29.749963Z","shell.execute_reply":"2024-11-11T14:51:29.755112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you already have the test_df loaded and the classifier trained (for example, a RandomForest, SVM, or MLP)\ntest_data = test_df\n# Tokenize the 'Utterance' column of the test dataset\ntokenized_test_data = test_data['Utterance'].apply(tokenize_function)\n\n# Extract input_ids and attention_mask for the test data\ninput_ids_test = torch.cat([x['input_ids'] for x in tokenized_test_data], dim=0)\nattention_mask_test = torch.cat([x['attention_mask'] for x in tokenized_test_data], dim=0)\n\n# Extract features for the test dataset\nfeatures_test = extract_bert_features(input_ids_test, attention_mask_test)\n\n# Convert features to numpy arrays for further processing (if required)\nfeatures_test_text = features_test.cpu().numpy()  # Move to CPU before converting to numpy\n\n# # If you have a trained classifier, use it to make predictions on the extracted features\n# y_test_pred = trained_classifier.predict(features_test_text)  # Assuming 'trained_classifier' is your trained model\n\n# # Now you can create a DataFrame with the predictions (if you need to submit or save them)\n# test_data['Predicted_Emotion'] = y_test_pred\n\n# # Save the predictions to a CSV file (optional)\n# test_data[['Utterance', 'Predicted_Emotion']].to_csv('predictions.csv', index=False)\n\n# print(\"Predictions saved to predictions.csv.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:22:44.469594Z","iopub.execute_input":"2024-11-11T14:22:44.470491Z","iopub.status.idle":"2024-11-11T14:22:44.896381Z","shell.execute_reply.started":"2024-11-11T14:22:44.47045Z","shell.execute_reply":"2024-11-11T14:22:44.89563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_test_text.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:22:45.463776Z","iopub.execute_input":"2024-11-11T14:22:45.46448Z","iopub.status.idle":"2024-11-11T14:22:45.470235Z","shell.execute_reply.started":"2024-11-11T14:22:45.464429Z","shell.execute_reply":"2024-11-11T14:22:45.46934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport numpy as np\n\ndef extract_features_for_row(row):\n    video_path = row['video_clip_path']\n    return extract_audio_features_from_video(video_path)\n\ndef extract_features_in_parallel(df):\n    # Parallel processing for extracting features from each video\n    results = Parallel(n_jobs=-1)(delayed(extract_features_for_row)(row) for _, row in df.iterrows())\n    \n    features = []\n    # No need to extract labels since test_df doesn't have 'Emotion' column\n    for result in results:\n        if result is not None:  # Skip invalid results\n            features.append(result)\n    \n    return np.array(features)\n\n# Usage example for test data:\nfeatures_audio = extract_features_in_parallel(test_df)\n\nprint(features_audio.shape)  # Check the shape of the extracted features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:18:03.730846Z","iopub.execute_input":"2024-11-11T14:18:03.7318Z","iopub.status.idle":"2024-11-11T14:18:36.382909Z","shell.execute_reply.started":"2024-11-11T14:18:03.731758Z","shell.execute_reply":"2024-11-11T14:18:36.381419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_audio.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:19:14.592047Z","iopub.execute_input":"2024-11-11T14:19:14.592498Z","iopub.status.idle":"2024-11-11T14:19:14.599424Z","shell.execute_reply.started":"2024-11-11T14:19:14.592457Z","shell.execute_reply":"2024-11-11T14:19:14.598537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Move the tensor from GPU to CPU before passing it to the classifier\n# features_test_cpu = features_a.cpu().numpy()  # Move to CPU and convert to numpy array\n# features_test_cpu.shape\ny_pred_audio = classifier.predict(features_audio)\ny_pred_mlp = mlp_model.predict(features_text)\ny_pred_vid = mlp_model_vid.predict(test_video_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:26:31.722461Z","iopub.execute_input":"2024-11-11T14:26:31.723386Z","iopub.status.idle":"2024-11-11T14:26:31.852097Z","shell.execute_reply.started":"2024-11-11T14:26:31.723343Z","shell.execute_reply":"2024-11-11T14:26:31.850725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_proba_audio = classifier.predict_proba(features_audio)\ny_pred_proba_text = mlp_model.predict_proba(features_test_text)\ny_pred_proba_vid = mlp_model_vid.predict_proba(test_video_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:23:35.544631Z","iopub.execute_input":"2024-11-11T14:23:35.545384Z","iopub.status.idle":"2024-11-11T14:23:35.66499Z","shell.execute_reply.started":"2024-11-11T14:23:35.545338Z","shell.execute_reply":"2024-11-11T14:23:35.66361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Weighted average of probabilities (set weights as needed)\ncombined_probs = 0.25 * y_pred_proba_audio + 0.7 * y_pred_proba_text + 0.05 * y_pred_proba_vid  # Adjust weights based on model trust\n\n# Get final predictions from combined probabilities\nfinal_predictions = np.argmax(combined_probs, axis=1)\ny_val = np.array(y_train, dtype=str)  # or dtype=int based on your data\nfinal_predictions = np.array(final_predictions, dtype=str)  # or dtype=int\n\n# # Evaluate the model\n# print(\"Weighted Fusion Model Accuracy:\", accuracy_score(y_val, final_predictions))\n# print(\"Weighted Fusion Model Classification Report:\\n\", classification_report(y_val, final_predictions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:53:39.178772Z","iopub.execute_input":"2024-11-11T14:53:39.179864Z","iopub.status.idle":"2024-11-11T14:53:39.186024Z","shell.execute_reply.started":"2024-11-11T14:53:39.179819Z","shell.execute_reply":"2024-11-11T14:53:39.185105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:53:42.028754Z","iopub.execute_input":"2024-11-11T14:53:42.029661Z","iopub.status.idle":"2024-11-11T14:53:42.035933Z","shell.execute_reply.started":"2024-11-11T14:53:42.0296Z","shell.execute_reply":"2024-11-11T14:53:42.035001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Emotion to integer mapping\nemotion_map = {'0':'anger', '1':'joy', '2':'neutral', '3':'sadness', '4':'surprise'}\n\n# Convert y_val and y_pred to integer labels\n# y_val_int = np.array([emotion_map[emotion] for emotion in y_val])\ny_pred_int = np.array([emotion_map[emotion] for emotion in final_predictions])\ny_pred_int","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:53:45.720704Z","iopub.execute_input":"2024-11-11T14:53:45.721118Z","iopub.status.idle":"2024-11-11T14:53:45.729257Z","shell.execute_reply.started":"2024-11-11T14:53:45.721078Z","shell.execute_reply":"2024-11-11T14:53:45.728147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_preds = y_pred_int\n\nall_ids = test_df[\"Sr No.\"]\n\nsubmission_df = pd.DataFrame({\n\n        'Sr No.': all_ids,\n\n        'Emotion': all_preds\n\n    })\n\n    \n\n# Save the DataFrame to CSV\n\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T14:53:46.382064Z","iopub.execute_input":"2024-11-11T14:53:46.383075Z","iopub.status.idle":"2024-11-11T14:53:46.391791Z","shell.execute_reply.started":"2024-11-11T14:53:46.383019Z","shell.execute_reply":"2024-11-11T14:53:46.390717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}